# ü§ñ Evaluaci√≥n de Respuestas de IA con Azure OpenAI y GitHub Actions üöÄ

Este proyecto utiliza GitHub Actions para automatizar la evaluaci√≥n de un modelo de lenguaje de Azure OpenAI. El proceso lee una serie de preguntas (prompts), genera una respuesta para cada una y luego las eval√∫a en funci√≥n de m√©tricas de calidad como la coherencia y la fluidez.

## ‚öôÔ∏è ¬øC√≥mo funciona?

El coraz√≥n de este proyecto es el workflow de GitHub Actions definido en `.github/workflows/evaluate.yml`. Este workflow se ejecuta autom√°ticamente con cada `push` a la rama `main` y sigue estos pasos:

1.  ‚ñ∂Ô∏è **Disparo Autom√°tico**: El workflow se inicia autom√°ticamente cada vez que se sube un commit a la rama `main`.

2.  üõ†Ô∏è **Configuraci√≥n del Entorno**: Se configura el entorno de ejecuci√≥n, instalando Python y las dependencias necesarias (`openai`, `pydantic`).

3.  ‚úçÔ∏è **Generaci√≥n de Respuestas**: Un script de Python utiliza las credenciales de Azure OpenAI para:
    *   Leer las preguntas del archivo `data/eval-input.jsonl`.
    *   Para cada pregunta, invocar al modelo de chat de Azure OpenAI para generar una respuesta.
    *   Guardar las preguntas junto con sus respuestas generadas en un nuevo archivo: `data/eval-input-generated.jsonl`.

4.  üìä **Evaluaci√≥n de Calidad**: Se utiliza la action `microsoft/genai-evals` para evaluar las respuestas generadas. Esta action:
    *   Utiliza un modelo de IA (el "juez") para evaluar la calidad de las respuestas del modelo de chat.
    *   Las m√©tricas de evaluaci√≥n utilizadas son `coherence` (coherencia) y `fluency` (fluidez).
    *   La configuraci√≥n de la evaluaci√≥n se define din√°micamente en el archivo `evaluate-config.json`.

5.  üì¶ **Publicaci√≥n de Resultados**: Los resultados de la evaluaci√≥n se guardan en un archivo `results.jsonl`, que se sube como un artefacto de la ejecuci√≥n del workflow. Este artefacto se puede descargar desde la p√°gina de resumen de la ejecuci√≥n.

## üöÄ Uso

Para utilizar este proyecto, sigue estos pasos:

### üîë 1. Configurar los Secretos

Este workflow requiere que configures los siguientes secretos en tu repositorio de GitHub (`Settings` > `Secrets and variables` > `Actions`):

*   `OIDC_AZURE_CLIENT_ID`: El ID de cliente de una identidad de Azure con permisos para autenticarse.
*   `OIDC_AZURE_TENANT_ID`: El ID del tenant de Azure.
*   `OIDC_AZURE_SUBSCRIPTION_ID`: El ID de la suscripci√≥n de Azure.
*   `AZURE_OPENAI_ENDPOINT`: La URL del endpoint de tu servicio Azure OpenAI.
*   `AZURE_OPENAI_API_KEY`: La clave de API para acceder al servicio.
*   `AZURE_OPENAI_API_VERSION`: La versi√≥n de la API de Azure OpenAI (ej. `2024-02-01`).
*   `AZURE_OPENAI_CHAT_DEPLOYMENT`: El nombre del despliegue de tu modelo de chat que quieres evaluar.

### üìù 2. Modificar los Datos de Entrada

Puedes a√±adir tus propias preguntas de evaluaci√≥n en el archivo `data/eval-input.jsonl`. El archivo debe ser de tipo [JSON Lines](https://jsonlines.org/), donde cada l√≠nea es un objeto JSON con, como m√≠nimo, una clave `"query"`:

```json
{"query": "¬øCu√°l es la capital de Francia?"}
{"query": "¬øQui√©n desarroll√≥ la teor√≠a de la relatividad?"}
```

### ‚ñ∂Ô∏è 3. Funcionamiento Autom√°tico

Simplemente haz un `git push` a la rama `main` despu√©s de tus commits. El workflow se ejecutar√° de forma autom√°tica. Puedes ver el progreso y los resultados en la pesta√±a **Actions** de tu repositorio.

### üìÑ 4. Revisar los Resultados

Una vez que la ejecuci√≥n del workflow haya finalizado, ve a la p√°gina de resumen de la ejecuci√≥n. En la secci√≥n de **Artifacts**, encontrar√°s un archivo llamado `evaluation-results` que puedes descargar. Este archivo contiene los resultados detallados de la evaluaci√≥n en formato JSON Lines.